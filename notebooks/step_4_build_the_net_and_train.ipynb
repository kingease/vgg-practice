{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('../datasets/')\n",
    "sys.path.append('../nets/')\n",
    "sys.path.append('../preprocessing/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import orientset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oriset = orientset.get_split('', '../data/tfrecord/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vgg_preprocessing, vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('vgg_orient/conv1/conv1_1',\n",
       "              <tf.Tensor 'vgg_orient/conv1/conv1_1/Relu:0' shape=(?, 224, 224, 64) dtype=float32>),\n",
       "             ('vgg_orient/conv1/conv1_2',\n",
       "              <tf.Tensor 'vgg_orient/conv1/conv1_2/Relu:0' shape=(?, 224, 224, 64) dtype=float32>),\n",
       "             ('vgg_orient/pool1',\n",
       "              <tf.Tensor 'vgg_orient/pool1/MaxPool:0' shape=(?, 112, 112, 64) dtype=float32>),\n",
       "             ('vgg_orient/conv2/conv2_1',\n",
       "              <tf.Tensor 'vgg_orient/conv2/conv2_1/Relu:0' shape=(?, 112, 112, 128) dtype=float32>),\n",
       "             ('vgg_orient/conv2/conv2_2',\n",
       "              <tf.Tensor 'vgg_orient/conv2/conv2_2/Relu:0' shape=(?, 112, 112, 128) dtype=float32>),\n",
       "             ('vgg_orient/pool2',\n",
       "              <tf.Tensor 'vgg_orient/pool2/MaxPool:0' shape=(?, 56, 56, 128) dtype=float32>),\n",
       "             ('vgg_orient/conv3/conv3_1',\n",
       "              <tf.Tensor 'vgg_orient/conv3/conv3_1/Relu:0' shape=(?, 56, 56, 256) dtype=float32>),\n",
       "             ('vgg_orient/conv3/conv3_2',\n",
       "              <tf.Tensor 'vgg_orient/conv3/conv3_2/Relu:0' shape=(?, 56, 56, 256) dtype=float32>),\n",
       "             ('vgg_orient/conv3/conv3_3',\n",
       "              <tf.Tensor 'vgg_orient/conv3/conv3_3/Relu:0' shape=(?, 56, 56, 256) dtype=float32>),\n",
       "             ('vgg_orient/pool3',\n",
       "              <tf.Tensor 'vgg_orient/pool3/MaxPool:0' shape=(?, 28, 28, 256) dtype=float32>),\n",
       "             ('vgg_orient/conv4/conv4_1',\n",
       "              <tf.Tensor 'vgg_orient/conv4/conv4_1/Relu:0' shape=(?, 28, 28, 512) dtype=float32>),\n",
       "             ('vgg_orient/conv4/conv4_2',\n",
       "              <tf.Tensor 'vgg_orient/conv4/conv4_2/Relu:0' shape=(?, 28, 28, 512) dtype=float32>),\n",
       "             ('vgg_orient/conv4/conv4_3',\n",
       "              <tf.Tensor 'vgg_orient/conv4/conv4_3/Relu:0' shape=(?, 28, 28, 512) dtype=float32>),\n",
       "             ('vgg_orient/pool4',\n",
       "              <tf.Tensor 'vgg_orient/pool4/MaxPool:0' shape=(?, 14, 14, 512) dtype=float32>),\n",
       "             ('vgg_orient/conv5/conv5_1',\n",
       "              <tf.Tensor 'vgg_orient/conv5/conv5_1/Relu:0' shape=(?, 14, 14, 512) dtype=float32>),\n",
       "             ('vgg_orient/conv5/conv5_2',\n",
       "              <tf.Tensor 'vgg_orient/conv5/conv5_2/Relu:0' shape=(?, 14, 14, 512) dtype=float32>),\n",
       "             ('vgg_orient/conv5/conv5_3',\n",
       "              <tf.Tensor 'vgg_orient/conv5/conv5_3/Relu:0' shape=(?, 14, 14, 512) dtype=float32>),\n",
       "             ('vgg_orient/pool5',\n",
       "              <tf.Tensor 'vgg_orient/pool5/MaxPool:0' shape=(?, 7, 7, 512) dtype=float32>),\n",
       "             ('vgg_orient/fc6',\n",
       "              <tf.Tensor 'vgg_orient/fc6/Relu:0' shape=(?, 1, 1, 4096) dtype=float32>),\n",
       "             ('vgg_orient/fc7',\n",
       "              <tf.Tensor 'vgg_orient/fc7/Relu:0' shape=(?, 1, 1, 4096) dtype=float32>),\n",
       "             ('vgg_orient/fc8',\n",
       "              <tf.Tensor 'vgg_orient/fc8/squeezed:0' shape=(?, 4) dtype=float32>)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view vgg16 net structure\n",
    "with tf.Graph().as_default():\n",
    "    VGG_IMAGE_SIZE = vgg.vgg_16.default_image_size\n",
    "    image_placeholder = tf.placeholder(tf.float32, [None, VGG_IMAGE_SIZE, VGG_IMAGE_SIZE, 3])\n",
    "    logits, end_points = vgg.vgg_16(image_placeholder, num_classes=oriset.num_classes, scope='vgg_orient')\n",
    "end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Squeeze_1:0\", shape=(?, 4), dtype=float32)\n",
      "Tensor(\"vgg_orient/fc8/squeezed:0\", shape=(?, 4), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./logs/model.ckpt-0\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # read train data\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(oriset)\n",
    "    image, label = data_provider.get(['image', 'label'])\n",
    "    VGG_IMAGE_SIZE = vgg.vgg_16.default_image_size\n",
    "    image = vgg_preprocessing.preprocess_for_train(image, VGG_IMAGE_SIZE, VGG_IMAGE_SIZE)\n",
    "    \n",
    "    # batch data\n",
    "    batch_image, batch_label = tf.train.batch([image, label], batch_size=16, allow_smaller_final_batch=True)\n",
    "    batch_one_hot_label = slim.one_hot_encoding(batch_label, oriset.num_classes)\n",
    "    batch_one_hot_label = tf.squeeze(batch_one_hot_label, [1])\n",
    "    \n",
    "    print(batch_one_hot_label)\n",
    "    # create the net\n",
    "    logits, _ = vgg.vgg_16(batch_image, num_classes=oriset.num_classes, scope='vgg_orient')\n",
    "    print(logits)\n",
    "    # create loss\n",
    "    total_loss = tf.losses.softmax_cross_entropy(batch_one_hot_label, logits)\n",
    "    \n",
    "    # create optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    # create train_op\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    \n",
    "    # start to learn\n",
    "    slim.learning.train(train_op, './logs/', log_every_n_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 已经具备保存功能了，但是这里的网络是从头训练\n",
    "# 事实上，我们不想从头训练，我们想\n",
    "# 1. 使用已经训练好的部分层的数据\n",
    "# 2. 对某些层我们不想再训练了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## display trainable variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- all tainable variables ----\n",
      "[<tf.Variable 'vgg_orient/conv1/conv1_1/weights:0' shape=(3, 3, 3, 64) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv1/conv1_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv1/conv1_2/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv1/conv1_2/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv2/conv2_1/weights:0' shape=(3, 3, 64, 128) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv2/conv2_1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv2/conv2_2/weights:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv2/conv2_2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_1/weights:0' shape=(3, 3, 128, 256) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_1/biases:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_2/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_2/biases:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_3/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_3/biases:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_1/weights:0' shape=(3, 3, 256, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_1/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_2/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_2/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_3/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_3/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_1/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_1/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_2/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_2/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_3/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_3/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'vgg_orient/fc6/weights:0' shape=(7, 7, 512, 4096) dtype=float32_ref>, <tf.Variable 'vgg_orient/fc6/biases:0' shape=(4096,) dtype=float32_ref>, <tf.Variable 'vgg_orient/fc7/weights:0' shape=(1, 1, 4096, 4096) dtype=float32_ref>, <tf.Variable 'vgg_orient/fc7/biases:0' shape=(4096,) dtype=float32_ref>, <tf.Variable 'vgg_orient/fc8/weights:0' shape=(1, 1, 4096, 4) dtype=float32_ref>, <tf.Variable 'vgg_orient/fc8/biases:0' shape=(4,) dtype=float32_ref>]\n",
      "---- filtered to train ----\n",
      "[<tf.Variable 'vgg_orient/fc6/weights:0' shape=(7, 7, 512, 4096) dtype=float32_ref>, <tf.Variable 'vgg_orient/fc6/biases:0' shape=(4096,) dtype=float32_ref>, <tf.Variable 'vgg_orient/fc7/weights:0' shape=(1, 1, 4096, 4096) dtype=float32_ref>, <tf.Variable 'vgg_orient/fc7/biases:0' shape=(4096,) dtype=float32_ref>, <tf.Variable 'vgg_orient/fc8/weights:0' shape=(1, 1, 4096, 4) dtype=float32_ref>, <tf.Variable 'vgg_orient/fc8/biases:0' shape=(4,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    VGG_IMAGE_SIZE = vgg.vgg_16.default_image_size\n",
    "    image_placeholder = tf.placeholder(tf.float32, [None, VGG_IMAGE_SIZE, VGG_IMAGE_SIZE, 3])\n",
    "    \n",
    "    # create the net\n",
    "    logits, _ = vgg.vgg_16(image_placeholder, num_classes=oriset.num_classes, scope='vgg_orient')\n",
    "    # display trainable variable\n",
    "    trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    print('---- all tainable variables ----')\n",
    "    print(trainable_variables)\n",
    "    \n",
    "    print('---- filtered to train ----')\n",
    "    # filter the variablee we want\n",
    "    scopes =['vgg_orient/fc6', 'vgg_orient/fc7', 'vgg_orient/fc8']\n",
    "    variables_to_train =[]\n",
    "    for scope in scopes:\n",
    "        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "        variables_to_train.extend(variables)\n",
    "    \n",
    "    print(variables_to_train)\n",
    "    # variable_to_train could set by parameters of variables  \n",
    "    # slim.learning.create_train_op( total_loss, optimizer, variables_to_train = variables_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load some layers for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'vgg_orient/conv1/conv1_1/weights:0' shape=(3, 3, 3, 64) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv1/conv1_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv1/conv1_2/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv1/conv1_2/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv2/conv2_1/weights:0' shape=(3, 3, 64, 128) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv2/conv2_1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv2/conv2_2/weights:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv2/conv2_2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_1/weights:0' shape=(3, 3, 128, 256) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_1/biases:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_2/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_2/biases:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_3/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv3/conv3_3/biases:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_1/weights:0' shape=(3, 3, 256, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_1/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_2/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_2/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_3/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv4/conv4_3/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_1/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_1/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_2/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_2/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_3/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'vgg_orient/conv5/conv5_3/biases:0' shape=(512,) dtype=float32_ref>]\n",
      "vgg_orient/conv1/conv1_1/weights\n",
      "vgg_orient/conv1/conv1_1/biases\n",
      "vgg_orient/conv1/conv1_2/weights\n",
      "vgg_orient/conv1/conv1_2/biases\n",
      "vgg_orient/conv2/conv2_1/weights\n",
      "vgg_orient/conv2/conv2_1/biases\n",
      "vgg_orient/conv2/conv2_2/weights\n",
      "vgg_orient/conv2/conv2_2/biases\n",
      "vgg_orient/conv3/conv3_1/weights\n",
      "vgg_orient/conv3/conv3_1/biases\n",
      "vgg_orient/conv3/conv3_2/weights\n",
      "vgg_orient/conv3/conv3_2/biases\n",
      "vgg_orient/conv3/conv3_3/weights\n",
      "vgg_orient/conv3/conv3_3/biases\n",
      "vgg_orient/conv4/conv4_1/weights\n",
      "vgg_orient/conv4/conv4_1/biases\n",
      "vgg_orient/conv4/conv4_2/weights\n",
      "vgg_orient/conv4/conv4_2/biases\n",
      "vgg_orient/conv4/conv4_3/weights\n",
      "vgg_orient/conv4/conv4_3/biases\n",
      "vgg_orient/conv5/conv5_1/weights\n",
      "vgg_orient/conv5/conv5_1/biases\n",
      "vgg_orient/conv5/conv5_2/weights\n",
      "vgg_orient/conv5/conv5_2/biases\n",
      "vgg_orient/conv5/conv5_3/weights\n",
      "vgg_orient/conv5/conv5_3/biases\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    VGG_IMAGE_SIZE = vgg.vgg_16.default_image_size\n",
    "    image_placeholder = tf.placeholder(tf.float32, [None, VGG_IMAGE_SIZE, VGG_IMAGE_SIZE, 3])\n",
    "    \n",
    "    # create the net\n",
    "    logits, _ = vgg.vgg_16(image_placeholder, num_classes=oriset.num_classes, scope='vgg_orient')\n",
    "    \n",
    "    # scopes for train\n",
    "    scopes =['vgg_orient/fc6', 'vgg_orient/fc7', 'vgg_orient/fc8']\n",
    "    \n",
    "    # get variables to restore\n",
    "    variables_to_restore = slim.get_variables_to_restore(exclude=scopes)\n",
    "    print(variables_to_restore)\n",
    "    for var in variables_to_restore:\n",
    "        print(var.op.name)\n",
    "    \n",
    "    # load the checkpoint which has different name with ours by mapping\n",
    "    variables_to_restore = { var.op.name.replace('vgg_orient', 'vgg_16') :var for var in variables_to_restore}\n",
    "    \n",
    "    checkpoint_path = '../checkpoints/vgg_16.ckpt'\n",
    "    slim.assign_from_checkpoint_fn(checkpoint_path,variables_to_restore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine transfering param and specifing train layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./logs/model.ckpt-99\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:global step 100: loss = 0.7037 (66.83 sec/step)\n",
      "INFO:tensorflow:global step 101: loss = 2.0884 (60.25 sec/step)\n",
      "INFO:tensorflow:global step 102: loss = 0.7757 (62.47 sec/step)\n",
      "INFO:tensorflow:global step 103: loss = 1.7277 (62.85 sec/step)\n",
      "INFO:tensorflow:global step 104: loss = 1.0007 (59.74 sec/step)\n",
      "INFO:tensorflow:global step 105: loss = 0.8898 (60.81 sec/step)\n",
      "INFO:tensorflow:global step 106: loss = 1.0514 (56.82 sec/step)\n",
      "INFO:tensorflow:global step 107: loss = 0.8572 (58.71 sec/step)\n",
      "INFO:tensorflow:global step 108: loss = 0.9160 (55.96 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.0149993\n",
      "INFO:tensorflow:global step 109: loss = 0.9540 (56.52 sec/step)\n",
      "INFO:tensorflow:global step 110: loss = 0.9955 (59.79 sec/step)\n",
      "INFO:tensorflow:global step 111: loss = 1.2882 (63.94 sec/step)\n",
      "INFO:tensorflow:global step 112: loss = 2.0837 (57.31 sec/step)\n",
      "INFO:tensorflow:global step 113: loss = 0.6324 (53.01 sec/step)\n",
      "INFO:tensorflow:global step 114: loss = 1.7149 (51.31 sec/step)\n",
      "INFO:tensorflow:global step 115: loss = 1.6026 (50.47 sec/step)\n",
      "INFO:tensorflow:global step 116: loss = 0.7515 (54.94 sec/step)\n",
      "INFO:tensorflow:global step 117: loss = 1.1730 (54.23 sec/step)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-28025049de68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# start to learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./logs/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_every_n_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/digitrecon/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/learning.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_op, logdir, train_step_fn, train_step_kwargs, log_every_n_steps, graph, master, is_chief, global_step, number_of_steps, init_op, init_feed_dict, local_init_op, init_fn, ready_op, summary_op, save_summaries_secs, summary_writer, startup_delay_steps, saver, save_interval_secs, sync_optimizer, session_config, trace_every_n_steps)\u001b[0m\n\u001b[1;32m    740\u001b[0m           \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m             total_loss, should_stop = train_step_fn(\n\u001b[0;32m--> 742\u001b[0;31m                 sess, train_op, global_step, train_step_kwargs)\n\u001b[0m\u001b[1;32m    743\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m               \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Stopping Training.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/digitrecon/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/learning.pyc\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(sess, train_op, global_step, train_step_kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m   total_loss, np_global_step = sess.run([train_op, global_step],\n\u001b[1;32m    483\u001b[0m                                         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrace_run_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                                         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    485\u001b[0m   \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/digitrecon/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/digitrecon/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/digitrecon/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/digitrecon/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/digitrecon/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # read train data\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(oriset)\n",
    "    image, label = data_provider.get(['image', 'label'])\n",
    "    VGG_IMAGE_SIZE = vgg.vgg_16.default_image_size\n",
    "    image = vgg_preprocessing.preprocess_for_train(image, VGG_IMAGE_SIZE, VGG_IMAGE_SIZE)\n",
    "    \n",
    "    # batch data\n",
    "    batch_image, batch_label = tf.train.batch([image, label], batch_size=32, allow_smaller_final_batch=True)\n",
    "    batch_one_hot_label = slim.one_hot_encoding(batch_label, oriset.num_classes)\n",
    "    batch_one_hot_label = tf.squeeze(batch_one_hot_label, [1])\n",
    "    \n",
    "    # create the training net\n",
    "    logits, _ = vgg.vgg_16(batch_image, num_classes=oriset.num_classes, scope='vgg_16', is_training=True)\n",
    "\n",
    "    # create loss\n",
    "    total_loss = tf.losses.softmax_cross_entropy(batch_one_hot_label, logits)\n",
    "        \n",
    "    # find the variablee we want to train\n",
    "    scopes =['vgg_16/fc6', 'vgg_16/fc7', 'vgg_16/fc8']\n",
    "    variables_to_train =[]\n",
    "    for scope in scopes:\n",
    "        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "        variables_to_train.extend(variables)\n",
    "    \n",
    "    # restore the specified layers' parameters\n",
    "    variables_to_restore = slim.get_variables_to_restore(exclude=scopes)\n",
    "    variables_to_restore = { var.op.name.replace('vgg_orient', 'vgg_16'):var for var in variables_to_restore}\n",
    "    \n",
    "    # create optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.00001, beta1=0.9, beta2=0.999)\n",
    "\n",
    "    # create train_op\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer, variables_to_train=variables_to_train)\n",
    "    \n",
    "    # restore parameters\n",
    "    init_fn = slim.assign_from_checkpoint_fn('../checkpoints/vgg_16.ckpt',variables_to_restore)\n",
    "    \n",
    "    # start to learn\n",
    "    slim.learning.train(train_op, './logs/', log_every_n_steps=1, init_fn=init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:digitrecon]",
   "language": "python",
   "name": "conda-env-digitrecon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
